---
title: "Analyzing Load Test Logs"
date: "2018-09-05"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analyzing Load Test Logs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "80%",
  fig.align = "center",
  fig.width = 8, 
  fig.height = 6
)

# unzip missing data folders
zips <- dir("./test_sessions", pattern = ".zip", full.names = TRUE)
folders_exist <- dir.exists(sub("\\.zip$", "", zips))
if (any(!folders_exist)) {
  lapply(zips[!folders_exist], unzip, exdir = "./test_sessions")
}
```

```{r setup, message = FALSE}
library(shinyloadtest)
library(dplyr)
```

## Loading data

Start by loading the `shinycannon` results into R with `load_runs()`. If you have a single run, just give it the directory of the results:

```{r, eval = FALSE}
df <- load_runs("test_sessions/demo1")
```

If you have multiple results, you can provide with multiple directories and informative labels:

```{r}
df <- load_runs(
  `1 user` = "test_sessions/demo1",
  `4 users` = "test_sessions/demo4",
  `16 users` = "test_sessions/demo16"
)
```

The result is a tidy data frame:

```{r}
df
```

The first three variables identify each simulated session:

* `run` labels each run of shinycannon.
* `session_id` uniquely identifies each simulated session within a run.
* `user_id` labels each simulated user (shinycannon worker) within a run.

You won't usually work with this data frame directly but it's available for custom analysis, and the variables are document in `?load_runs`.

## Report output

Once you have the data, you'll typically use it to create a standalone HTML report:

```{r, eval = FALSE}
shinyloadtest_report(df, "report.html")
```

This report contains six major sections of diagnostic information: sessions, session duration, event waterfall, latency, event duration, and event concurrency. I describe each in turn below.

## Sessions

The **Sessions** tab of displays the sessions performed by each shinycannon worker. Each row represent one worker and each rectangle represents the time taken to perform an action. The following plot shows the actions performed by workers over 5 minutes (600s).

```{r, echo=FALSE, fig.height=2.5}
df %>%
  filter(run == "4 users") %>%
  slt_user()
```

If you use more workers, you'll see more rows. The following plot uses 16 simultaneous workers:

```{r, echo = FALSE, fig.height = 5}
df %>%
  filter(run == "16 users") %>%
  slt_user()
```

There are five types of action:

* Homepage (red): first, the browser must load the HTML homepage generated
  by your `ui`. This represents the start of a new simulated session.
  
* JS/CSS (orange): once the browser has read the homepage, it next 
  requests the `.css` and `.js` files needed to render the page.

* Start session (green): now the browser creates a bi-direction line of
  communication to the Shiny session using [SockJS](http://sockjs.org). 
  The browser user this to notify Shiny when the user interacts with an
  input, and Shiny uses it to tell the browser how to update the outputs.

* Calculate (blue): the user has changed and input and shiny has performed
  computation to update an output or execute an observer.
  
* User thinking (uncoloured): the app is waiting for input; i.e. the user
  is thinking about what to do next.

Also note the dotted vertical lines which show the major phases of the load test:

* The creation of new sessions is staggered over a **warmup** period.

* Once the targeted number of concurrent sessions is reached, the
  **maintenance** begins, and the number of session is kept constant.
  
* After the desired test duration is reached, user sessions begin ending
  during the **cooldown**.

## Session duration

The main focus of the **Sessions** tab is how the experimental design: how the sessions were divided across workers. Generally, you'll want to spend more time with the **Session duration** tab which better summarises the experience of a simulated user. The session duration plot has one row per session:

```{r, echo = FALSE}
df %>% 
  filter(run == "4 users") %>% 
  slt_session_duration() + 
  ggplot2::labs(title = "4 simulatenous users")
```

Here the red line represents the running time of the original script. So this plot shows us that most sessions are completed in about the same amount of time as the original script, which suggests that the app can comfortably handle four simultaneous users.

We get a very different picture if we look at the performance of the app with 16 simultaneous users:

```{r, echo = FALSE}
df %>% 
  filter(run == "16 users") %>% 
  slt_session_duration() + 
  ggplot2::labs(title = "16 simultaneous users")
```

Note that the fastest session now takes ~4 times longer than the original recording, and there's one session that's twice again as long as that. This suggests that server resource contention might be occurring.

## Event Waterfall

The **Event Waterfall** gives a more precise view into the individual events within a recording. Each line represents a session, with events running in row from top to bottom, and elapsed time on the x-axis.

```{r, echo = FALSE, fig.height=7}
df %>% 
  filter(run == "4 users") %>% 
  slt_waterfall()
```

In this plot you're primarily looking for uniform shape. In particular you want the lines to be parallel, because a crossing line indicates that user A started before user B, but finished afterwards (this is like seeing the table that ordered after you get their food first).

If we look at another run where a file was uploaded and many POST requests were made for a DT table, we can see many line crossings. This happens when execution for that step took **much** more time than the other sessions.  

```{r, echo = FALSE, fig.height = 16}
upload <- load_runs("./test_sessions/upload-dt/", verbose = FALSE)
upload %>% slt_waterfall()
```

## Latency

The **Latency** tab displays two plots. The first plot displays the total HTTP (homepage + js/css) time for each session:

```{r, echo = FALSE, fig.height=7}
df %>% 
  filter(run == "4 users") %>% 
  slt_http_latency()
```

The second plot displays the maximum calculation time:

```{r, echo = FALSE, fig.height=7}
df %>% 
  filter(run == "4 users") %>% 
  slt_websocket_latency()
```

Both plots display a cutoff value, that defaults to 5s but can be overriden with the `cutoff` parameters to `shinyloadtest_report()`.

The total HTTP latency plot helps to diagnose how long a user has to wait before a website has appeared on their browser.  Ideally, this should be no time at all, as the server is just providing static files.

The maximum SockJS latency plot helps to diagnose the maximum amount of time a user has to wait before an input is updated or output is recalculated.  Ideally, this should be shorter than our attention spans.


In the example above, the `16 users` run has a very strong pattern in it's maximum calculation time.  Knowing that the `16 users` run has the same recording as the `1 user` and `4 users` runs, it would appear that the `16 users` server is under too much load as the same calculations should be computed within 1 second (as in the `1 user` run).


## Event Duration

The `Event Duration` tab has three to four tabs available to choose from:

* Slowest max time - Event plots are arranged by the slowest maximum time within each plot
* Slowest min time - Event plots are arranged by the slowest minimum time within each plot
* Largest mean difference - Event plots are arranged by the largest difference in the run's mean event time. This option is available if more than one run is being displayed.
* Data table - A tabular display of the slowest max time, slowest min time, and (possibly) largest mean difference

```{r, echo = FALSE}
df %>% filter(input_line_number == 1) %>% slt_time_boxplot()
```

Ideally, the boxplots should not differ for any event and are close to 0 seconds elapsed.  As resource contention goes up, event time might go up.  Hopefully, there are no extreme outliers above the center mass of the boxplot as shown below.

```{r, echo = FALSE}
df %>% filter(input_line_number == 4) %>% slt_time_boxplot()
```

## Event Concurrency

The `Event Concurrency` tab has four tabs available to choose from:

* Largest slope - Event plots are arranged by the largest slope magnitude found when fitting a line to each run
* Largest intercept - Event plots are arranged by the largest intercept magnitude found when fitting a line to each run
* Largest error - Event plots are arranged by the largest error magnitude found when fitting a line to each run
* Data table - A tabular display of the raw slope, raw intercept, and raw error values

Ideally, we want a flat linear model with no slope.  This means that the calculations of our shiny application will scale as user concurrency goes up.

```{r, echo = FALSE}
df %>% filter(input_line_number == 16) %>% slt_time_concurrency()
```

However, if a strong linear trend exists as concurrency is increased, users will be waiting longer as more users are accessing the shiny application.

```{r, echo = FALSE}
df %>% filter(input_line_number == 34) %>% slt_time_concurrency()
```


<!-- # Tips -->

<!-- * It is always good to establish a baseline or an __approved__ run when comparing against potential server configuration runs.  This will give you a baseline to compare against.   -->

<!-- * Try to only alter one server configuration component at a time when deviating from your baseline run.  This helps pinpoint exactly what caused a speedup or slowdown in the application response time.   -->


## Underlying data

There are a few types of log files generated by a `shinycannon` recording session.

* `./recording.log`
  * Copy of the original recording used
  * The file is comprised of JSON lines that capture all of the HTTP(s) and SockJS traffic that occurred during the recording
* `./detail.log`
  * Log output to help debug errors
* `./sessions/*.csv`
  * Collection of files for each session number, simulated user id, and user session
  * One file is generated for each simulated session
  * The file name format displays `SESSION_USER_USERSESSION.csv`
  * Lines starting with `#` should be ignored when processing

These are processed `load_runs()` to generate a tidy data frame with the following columns:



They key idea is that each row of the tidy results data frame represents an event. Events include the initial page load, user inputs and their associated outputs, and user wait times. You'll want to compare the duration of event times across different load tests.

