---
title: "Analyzing Load Test Logs"
date: "2018-09-05"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analyzing Load Test Logs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%",
  fig.width = 8,
  fig.height = 6
)

# unzip missing data folders
zips <- dir("./test_sessions", pattern = ".zip", full.names = TRUE)
folders_exist <- dir.exists(sub("\\.zip$", "", zips))
if (any(!folders_exist)) {
  lapply(zips[!folders_exist], unzip, exdir = "./test_sessions")
}

ggplot2::theme_set(ggplot2::theme_grey(12))
```

```{r setup, message = FALSE}
library(shinyloadtest)
library(dplyr)
```

## Loading data

Start by loading the `shinycannon` results into R with `load_runs()`. If you have a single run, just give it the directory of the results:

```{r, eval = FALSE}
df <- load_runs("test_sessions/demo1")
```

If you have multiple results, you can provide with multiple directories and informative labels:

```{r}
df <- load_runs(
  `1 user` = "test_sessions/demo1",
  `4 users` = "test_sessions/demo4",
  `16 users` = "test_sessions/demo16"
)
```

The result is a tidy data frame:

```{r}
df
```

The first three variables identify each simulated session:

* `run` labels each run of shinycannon.
* `session_id` uniquely identifies each simulated session within a run.
* `user_id` labels each simulated user (shinycannon worker) within a run.

You won't usually work with this data frame directly but it's available for custom analysis, and the variables are document in `?load_runs`.

## Report output

Once you have the data, you'll typically use it to create a standalone HTML report:

```{r, eval = FALSE}
shinyloadtest_report(df, "report.html")
```

This report contains six major sections of diagnostic information: sessions, session duration, event waterfall, latency, event duration, and event concurrency. I describe each in turn below.

## Sessions

The **Sessions** page of displays the sessions performed by each shinycannon worker. Each row represent one worker and each rectangle represents the time taken to perform an action. The following plot shows the actions performed by workers over 5 minutes (600s).

```{r, echo=FALSE, fig.height=2.5}
df %>%
  filter(run == "4 users") %>%
  slt_user()
```

If you use more workers, you'll see more rows. The following plot uses 16 simultaneous workers:

```{r, echo = FALSE, fig.height = 5}
df %>%
  filter(run == "16 users") %>%
  slt_user()
```

There are five types of action:

* Homepage (red): first, the browser must load the HTML homepage generated
  by your `ui`. This represents the start of a new simulated session.
  
* JS/CSS (orange): once the browser has read the homepage, it next 
  requests the `.css` and `.js` files needed to render the page.

* Start session (green): now the browser creates a bi-direction line of
  communication to the Shiny session using [SockJS](http://sockjs.org). 
  The browser user this to notify Shiny when the user interacts with an
  input, and Shiny uses it to tell the browser how to update the outputs.

* Calculate (blue): the user has changed and input and shiny has performed
  computation to update an output or execute an observer.
  
* User thinking (uncoloured): the app is waiting for input; i.e. the user
  is thinking about what to do next.

Also note the dotted vertical lines which show the major phases of the load test:

* The creation of new sessions is staggered over a **warmup** period.

* Once the targeted number of concurrent sessions is reached, the
  **maintenance** begins, and the number of session is kept constant.
  
* After the desired test duration is reached, user sessions begin ending
  during the **cooldown**.

## Session duration

The main focus of the **Sessions** page is how the experimental design: how the sessions were divided across workers. Generally, you'll want to spend more time with the **Session duration** page which better summarises the experience of a simulated user. The session duration plot has one row per session:

```{r, echo = FALSE}
df %>% 
  filter(run == "4 users") %>% 
  slt_session_duration() + 
  ggplot2::labs(title = "4 simulatenous users")
```

Here the red line represents the running time of the original script. So this plot shows us that most sessions are completed in about the same amount of time as the original script, which suggests that the app can comfortably handle four simultaneous users.

We get a very different picture if we look at the performance of the app with 16 simultaneous users:

```{r, echo = FALSE}
df %>% 
  filter(run == "16 users") %>% 
  slt_session_duration() + 
  ggplot2::labs(title = "16 simultaneous users")
```

Note that the fastest session now takes ~4 times longer than the original recording, and there's one session that's twice again as long as that. This suggests that server resource contention might be occurring.

## Event Waterfall

The **Event Waterfall** gives a more precise view into the individual events within a recording. Each line represents a session, with events running from top to bottom, and elapsed time on the x-axis.

```{r, echo = FALSE, fig.height=7}
df %>% 
  filter(run == "4 users") %>% 
  slt_waterfall()
```

In this plot you're primarily looking for uniform shape. In particular you want the lines to be parallel, because a crossing line indicates that user A started before user B, but finished afterwards (this is like seeing the table that ordered after you get their food first).

If we look at another run where a file was uploaded and many POST requests were made for a DT table, we can see many line crossings. This happens when execution for that step took **much** more time than the other sessions.  

```{r, echo = FALSE, fig.height = 10}
upload <- load_runs("./test_sessions/upload-dt/", verbose = FALSE)
upload %>% slt_waterfall()
```

## Latency

The **Latency** page displays two plots. The first plot displays the total HTTP (homepage + js/css) time for each session:

```{r, echo = FALSE}
df %>% 
  filter(run == "4 users") %>% 
  slt_http_latency()
```

This plot helps diagnose how long a user has to wait before they see _something_ in their browser. Ideally, this will be very short since the server is just providing static files.

The second plot displays the maximum calculation time:

```{r, echo = FALSE}
df %>% 
  filter(run == "4 users") %>% 
  slt_websocket_latency()
```

This plot helps to diagnose how much time a user spends waiting for outputs to be updated. 

## Event Duration

The **Event Duration** page helps understand the variability of individual events. Each event is summarised with a boxplot, with run on the x-axis and time on the y-axis:

```{r, echo = FALSE}
df %>% filter(input_line_number == 1) %>% slt_time_boxplot()
```

Since there are typically many events, and most are uninteresting, the report gives you three ways of sorting to find interesting events:

* Slowest maximum time
* Slowest minimum time
* Largest mean difference (only shown if more than one run being processed).

These plots can help you narrow in on particularly slow events, but you should also be interested in events with high variability.

## Event Concurrency

The **Event Concurrency** page helps you understand how events responded to varying levels of concurrency. Each simulated session is represented by a point with number of concurrent processes on the x-axis and event time on the y-axis. The points are coloured by run. Each run also gets a line of best fit, which gives a sense for how the event respond to increasing concurrency. 

```{r, echo = FALSE}
df %>% filter(input_line_number == 16) %>% slt_time_concurrency()
```

Like **Event Duration**, most events are uninteresting, so the report gives you three ways of sorting to find interesting events:

* Largest slope, which emphasises events that appear to respond particularly poorly to increasing concurrency.
* Largest intercept
* Largest error

If you want your app to handle as many user as possible, look for events where the time increases steeply with concurrency - these are places where optimisation will have the greatest impact.
